{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Self Consistency Prompting\n",
    "\n",
    "One of the more advanced techniques in prompt engineering is self-consistency, introduced by `Wang et al. (2022)`. \n",
    "\n",
    "This method seeks to improve upon the traditional greedy decoding typically used in chain-of-thought (CoT) prompting. \n",
    "\n",
    "The core concept involves sampling multiple diverse reasoning paths through few-shot CoT and leveraging these variations to determine the most consistent answer. The technique  enhances the effectiveness of CoT prompting, particularly for tasks requiring arithmetic and commonsense reasoning.\n",
    "\n",
    "## References:\n",
    "* [Wang et al. (2022)](https://arxiv.org/abs/2203.11171)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running this code on MyBind.org\n",
    "\n",
    "Note: remember that you will need to **adjust CONFIG** with **proper URL and API_KEY**!\n",
    "\n",
    "[![Binder](https://mybinder.org/badge_logo.svg)](https://mybinder.org/v2/gh/GenILab-FAU/prompt-eng/HEAD?urlpath=%2Fdoc%2Ftree%2Fprompt-eng%2Fself_consistency.ipynb)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To calculate this, we need to know that the logarithm of 2 (base 10) is approximately 0.301.\n",
      "\n",
      "So,\n",
      "\n",
      "984 * log(2) ≈ 984 * 0.301\n",
      "≈ 295.584\n",
      "Time taken: 5.399s\n"
     ]
    }
   ],
   "source": [
    "##\n",
    "## ZERO SHOT PROMPTING\n",
    "##\n",
    "\n",
    "from _pipeline import create_payload, model_req\n",
    "\n",
    "#### (1) Adjust the inbounding  Prompt, simulating inbounding requests from users or other systems\n",
    "MESSAGE = \"What is 984 * log(2)\"\n",
    "\n",
    "#### (2) Adjust the Prompt Engineering Technique to be applied, simulating Workflow Templates\n",
    "\n",
    "## @TODO \n",
    "PROMPT = MESSAGE \n",
    "\n",
    "#### (3) Configure the Model request, simulating Workflow Orchestration\n",
    "# Documentation: https://github.com/ollama/ollama/blob/main/docs/api.md\n",
    "payload = create_payload(target=\"ollama\",\n",
    "                         model=\"llama3.2:latest\", \n",
    "                         prompt=PROMPT, \n",
    "                         temperature=1.0, \n",
    "                         num_ctx=100, \n",
    "                         num_predict=100)\n",
    "\n",
    "### YOU DONT NEED TO CONFIGURE ANYTHING ELSE FROM THIS POINT\n",
    "# Send out to the model\n",
    "time, response = model_req(payload=payload)\n",
    "print(response)\n",
    "if time: print(f'Time taken: {time}s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response:\n",
      "To find the value of \\(984 \\times \\log(2)\\), we first need to calculate the value of \\(\\log(2)\\). The base of the logarithm is not specified, so we will assume that it is a common logarithm with base 10. \n",
      "\n",
      "\\[\\log(2) \\approx 0.3010\\]\n",
      "\n",
      "Now, to find \\(984 \\times \\log(2)\\):\n",
      "\n",
      "\\[984 \\times 0.3010 = 295.784\n",
      "Time taken: 1.05s\n"
     ]
    }
   ],
   "source": [
    "import openai\n",
    "import time\n",
    "\n",
    "#  OpenAI API key\n",
    "openai.api_key = \"sk-n6UrvoaItfPkH35KyG1D0e78rIeDGUb9_g5jgnRHeT3BlbkFJGEGJfnh39Ny3CT2myPz7hhdXwSx2qlfNduTuqN0dkA\"\n",
    "\n",
    "# Inbound message (this simulates a user/system request)\n",
    "MESSAGE = \"What is 984 * log(2)\"\n",
    "\n",
    "# For zero-shot prompting, we use the inbound message directly as the prompt.\n",
    "PROMPT = MESSAGE\n",
    "\n",
    "# Start timing the request\n",
    "start_time = time.time()\n",
    "\n",
    "# Call the OpenAI ChatCompletion API using GPT-3.5-turbo\n",
    "response = openai.ChatCompletion.create(\n",
    "    model=\"gpt-3.5-turbo\",\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful math tutor.\"},\n",
    "        {\"role\": \"user\", \"content\": PROMPT}\n",
    "    ],\n",
    "    max_tokens=100,\n",
    "    temperature=1.0,\n",
    ")\n",
    "\n",
    "# End timing\n",
    "end_time = time.time()\n",
    "elapsed = end_time - start_time\n",
    "\n",
    "# Extract the model's response\n",
    "result = response.choices[0].message['content'].strip()\n",
    "\n",
    "# Print the response and time taken\n",
    "print(\"Response:\")\n",
    "print(result)\n",
    "print(f\"Time taken: {elapsed:.2f}s\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
